{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhsesteves/trabalhotecnologiasavancadas/blob/master/Pr%C3%A1tica_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JZk_HIg62q8i"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('[INFO] accessing MNIST...')\n",
        "((trainX, trainY), (testX, testY)) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC_zCHGk3Dad",
        "outputId": "2a08001e-458f-47b7-dad0-6023a32421fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] accessing MNIST...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(9):\n",
        " # define subplot\n",
        " plt.subplot(330 + 1 + i)\n",
        " # plot raw pixel datad\n",
        " plt.imshow(trainX[i].reshape(28,28), cmap=plt.get_cmap('gray'))\n",
        "# show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "ZsK3qtrq7PLm",
        "outputId": "2f717781-102c-4e66-ebb5-ac9509a63869"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainX[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "NVDRUl2w7UsX",
        "outputId": "0ec2a021-c99c-4981-af85-d41d89921b22"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-899185ba-d5e3-440d-8d5d-1258fff41b4c\" class=\"ndarray_repr\"><pre>ndarray (28, 28) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-899185ba-d5e3-440d-8d5d-1258fff41b4c button').onclick = (e) => {\n",
              "        document.querySelector('#id-899185ba-d5e3-440d-8d5d-1258fff41b4c').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-899185ba-d5e3-440d-8d5d-1258fff41b4c button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainX.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ygT0Of19e4a",
        "outputId": "4beb3f05-a0ed-4c73-e694-ad3fd1d85688"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNWlnBg19eq8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX = trainX.reshape((trainX.shape[0], 28 * 28 * 1))\n",
        "testX = testX.reshape((testX.shape[0], 28 * 28 * 1))\n",
        "trainX = trainX.astype('float32') / 255.0\n",
        "testX = testX.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "iIeF0EHJ3JJb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlJ4CM31742h",
        "outputId": "67b92c12-0ae1-4da1-b772-7f34fd80c9e2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
              "       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
              "       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n",
              "       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n",
              "       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n",
              "       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n",
              "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n",
              "       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
              "       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.54509807,\n",
              "       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n",
              "       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n",
              "       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n",
              "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
              "       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n",
              "       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n",
              "       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n",
              "       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
              "       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n",
              "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n",
              "       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n",
              "       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainX.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skvN97CB6SRl",
        "outputId": "3a3d5e5e-b9b1-4e15-a189-3b50da3989c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainY.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95-Is8yS6mPc",
        "outputId": "949d45f0-1b22-46a2-b796-f77dbe3b304b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testX.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwQZlDF67j9M",
        "outputId": "03059c7a-d291-448d-a5a4-999825784263"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainY[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsYDlbDK93v3",
        "outputId": "dc000ecc-c74f-4e73-d5e7-86bfa10152a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.transform(testY)"
      ],
      "metadata": {
        "id": "NbkPh-el3Otq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainY.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEvaPeDD-JFU",
        "outputId": "8404a6a0-b955-4fb6-c869-e530a95ca442"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainY[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1Ip9ckx-Q4H",
        "outputId": "ac9d0d30-8027-4de2-eba7-94f24da11e2e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(256, input_shape=(784,), activation='sigmoid'))\n",
        "model.add(Dense(128, activation='sigmoid'))\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "xr78uMM_3SDx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(model.fit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnUXzlyaDlmr",
        "outputId": "451cc123-c5e5-4631-9440-aa0a21006c37"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method fit in module keras.src.engine.training:\n",
            "\n",
            "fit(x=None, y=None, batch_size=None, epochs=1, verbose='auto', callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) method of keras.src.engine.sequential.Sequential instance\n",
            "    Trains the model for a fixed number of epochs (dataset iterations).\n",
            "    \n",
            "    Args:\n",
            "        x: Input data. It could be:\n",
            "          - A Numpy array (or array-like), or a list of arrays\n",
            "            (in case the model has multiple inputs).\n",
            "          - A TensorFlow tensor, or a list of tensors\n",
            "            (in case the model has multiple inputs).\n",
            "          - A dict mapping input names to the corresponding array/tensors,\n",
            "            if the model has named inputs.\n",
            "          - A `tf.data` dataset. Should return a tuple\n",
            "            of either `(inputs, targets)` or\n",
            "            `(inputs, targets, sample_weights)`.\n",
            "          - A generator or `keras.utils.Sequence` returning `(inputs,\n",
            "            targets)` or `(inputs, targets, sample_weights)`.\n",
            "          - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n",
            "            callable that takes a single argument of type\n",
            "            `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n",
            "            `DatasetCreator` should be used when users prefer to specify the\n",
            "            per-replica batching and sharding logic for the `Dataset`.\n",
            "            See `tf.keras.utils.experimental.DatasetCreator` doc for more\n",
            "            information.\n",
            "          A more detailed description of unpacking behavior for iterator\n",
            "          types (Dataset, generator, Sequence) is given below. If these\n",
            "          include `sample_weights` as a third component, note that sample\n",
            "          weighting applies to the `weighted_metrics` argument but not the\n",
            "          `metrics` argument in `compile()`. If using\n",
            "          `tf.distribute.experimental.ParameterServerStrategy`, only\n",
            "          `DatasetCreator` type is supported for `x`.\n",
            "        y: Target data. Like the input data `x`,\n",
            "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
            "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
            "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
            "          or `keras.utils.Sequence` instance, `y` should\n",
            "          not be specified (since targets will be obtained from `x`).\n",
            "        batch_size: Integer or `None`.\n",
            "            Number of samples per gradient update.\n",
            "            If unspecified, `batch_size` will default to 32.\n",
            "            Do not specify the `batch_size` if your data is in the\n",
            "            form of datasets, generators, or `keras.utils.Sequence`\n",
            "            instances (since they generate batches).\n",
            "        epochs: Integer. Number of epochs to train the model.\n",
            "            An epoch is an iteration over the entire `x` and `y`\n",
            "            data provided\n",
            "            (unless the `steps_per_epoch` flag is set to\n",
            "            something other than None).\n",
            "            Note that in conjunction with `initial_epoch`,\n",
            "            `epochs` is to be understood as \"final epoch\".\n",
            "            The model is not trained for a number of iterations\n",
            "            given by `epochs`, but merely until the epoch\n",
            "            of index `epochs` is reached.\n",
            "        verbose: 'auto', 0, 1, or 2. Verbosity mode.\n",
            "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            "            'auto' becomes 1 for most cases, but 2 when used with\n",
            "            `ParameterServerStrategy`. Note that the progress bar is not\n",
            "            particularly useful when logged to a file, so verbose=2 is\n",
            "            recommended when not running interactively (eg, in a production\n",
            "            environment). Defaults to 'auto'.\n",
            "        callbacks: List of `keras.callbacks.Callback` instances.\n",
            "            List of callbacks to apply during training.\n",
            "            See `tf.keras.callbacks`. Note\n",
            "            `tf.keras.callbacks.ProgbarLogger` and\n",
            "            `tf.keras.callbacks.History` callbacks are created automatically\n",
            "            and need not be passed into `model.fit`.\n",
            "            `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
            "            `verbose` argument to `model.fit`.\n",
            "            Callbacks with batch-level calls are currently unsupported with\n",
            "            `tf.distribute.experimental.ParameterServerStrategy`, and users\n",
            "            are advised to implement epoch-level calls instead with an\n",
            "            appropriate `steps_per_epoch` value.\n",
            "        validation_split: Float between 0 and 1.\n",
            "            Fraction of the training data to be used as validation data.\n",
            "            The model will set apart this fraction of the training data,\n",
            "            will not train on it, and will evaluate\n",
            "            the loss and any model metrics\n",
            "            on this data at the end of each epoch.\n",
            "            The validation data is selected from the last samples\n",
            "            in the `x` and `y` data provided, before shuffling. This\n",
            "            argument is not supported when `x` is a dataset, generator or\n",
            "            `keras.utils.Sequence` instance.\n",
            "            If both `validation_data` and `validation_split` are provided,\n",
            "            `validation_data` will override `validation_split`.\n",
            "            `validation_split` is not yet supported with\n",
            "            `tf.distribute.experimental.ParameterServerStrategy`.\n",
            "        validation_data: Data on which to evaluate\n",
            "            the loss and any model metrics at the end of each epoch.\n",
            "            The model will not be trained on this data. Thus, note the fact\n",
            "            that the validation loss of data provided using\n",
            "            `validation_split` or `validation_data` is not affected by\n",
            "            regularization layers like noise and dropout.\n",
            "            `validation_data` will override `validation_split`.\n",
            "            `validation_data` could be:\n",
            "              - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n",
            "              - A tuple `(x_val, y_val, val_sample_weights)` of NumPy\n",
            "                arrays.\n",
            "              - A `tf.data.Dataset`.\n",
            "              - A Python generator or `keras.utils.Sequence` returning\n",
            "              `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n",
            "            `validation_data` is not yet supported with\n",
            "            `tf.distribute.experimental.ParameterServerStrategy`.\n",
            "        shuffle: Boolean (whether to shuffle the training data\n",
            "            before each epoch) or str (for 'batch'). This argument is\n",
            "            ignored when `x` is a generator or an object of tf.data.Dataset.\n",
            "            'batch' is a special option for dealing\n",
            "            with the limitations of HDF5 data; it shuffles in batch-sized\n",
            "            chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
            "        class_weight: Optional dictionary mapping class indices (integers)\n",
            "            to a weight (float) value, used for weighting the loss function\n",
            "            (during training only).\n",
            "            This can be useful to tell the model to\n",
            "            \"pay more attention\" to samples from\n",
            "            an under-represented class. When `class_weight` is specified\n",
            "            and targets have a rank of 2 or greater, either `y` must be\n",
            "            one-hot encoded, or an explicit final dimension of `1` must\n",
            "            be included for sparse class labels.\n",
            "        sample_weight: Optional Numpy array of weights for\n",
            "            the training samples, used for weighting the loss function\n",
            "            (during training only). You can either pass a flat (1D)\n",
            "            Numpy array with the same length as the input samples\n",
            "            (1:1 mapping between weights and samples),\n",
            "            or in the case of temporal data,\n",
            "            you can pass a 2D array with shape\n",
            "            `(samples, sequence_length)`,\n",
            "            to apply a different weight to every timestep of every sample.\n",
            "            This argument is not supported when `x` is a dataset, generator,\n",
            "            or `keras.utils.Sequence` instance, instead provide the\n",
            "            sample_weights as the third element of `x`.\n",
            "            Note that sample weighting does not apply to metrics specified\n",
            "            via the `metrics` argument in `compile()`. To apply sample\n",
            "            weighting to your metrics, you can specify them via the\n",
            "            `weighted_metrics` in `compile()` instead.\n",
            "        initial_epoch: Integer.\n",
            "            Epoch at which to start training\n",
            "            (useful for resuming a previous training run).\n",
            "        steps_per_epoch: Integer or `None`.\n",
            "            Total number of steps (batches of samples)\n",
            "            before declaring one epoch finished and starting the\n",
            "            next epoch. When training with input tensors such as\n",
            "            TensorFlow data tensors, the default `None` is equal to\n",
            "            the number of samples in your dataset divided by\n",
            "            the batch size, or 1 if that cannot be determined. If x is a\n",
            "            `tf.data` dataset, and 'steps_per_epoch'\n",
            "            is None, the epoch will run until the input dataset is\n",
            "            exhausted.  When passing an infinitely repeating dataset, you\n",
            "            must specify the `steps_per_epoch` argument. If\n",
            "            `steps_per_epoch=-1` the training will run indefinitely with an\n",
            "            infinitely repeating dataset.  This argument is not supported\n",
            "            with array inputs.\n",
            "            When using `tf.distribute.experimental.ParameterServerStrategy`:\n",
            "              * `steps_per_epoch=None` is not supported.\n",
            "        validation_steps: Only relevant if `validation_data` is provided and\n",
            "            is a `tf.data` dataset. Total number of steps (batches of\n",
            "            samples) to draw before stopping when performing validation\n",
            "            at the end of every epoch. If 'validation_steps' is None,\n",
            "            validation will run until the `validation_data` dataset is\n",
            "            exhausted. In the case of an infinitely repeated dataset, it\n",
            "            will run into an infinite loop. If 'validation_steps' is\n",
            "            specified and only part of the dataset will be consumed, the\n",
            "            evaluation will start from the beginning of the dataset at each\n",
            "            epoch. This ensures that the same validation samples are used\n",
            "            every time.\n",
            "        validation_batch_size: Integer or `None`.\n",
            "            Number of samples per validation batch.\n",
            "            If unspecified, will default to `batch_size`.\n",
            "            Do not specify the `validation_batch_size` if your data is in\n",
            "            the form of datasets, generators, or `keras.utils.Sequence`\n",
            "            instances (since they generate batches).\n",
            "        validation_freq: Only relevant if validation data is provided.\n",
            "          Integer or `collections.abc.Container` instance (e.g. list, tuple,\n",
            "          etc.).  If an integer, specifies how many training epochs to run\n",
            "          before a new validation run is performed, e.g. `validation_freq=2`\n",
            "          runs validation every 2 epochs. If a Container, specifies the\n",
            "          epochs on which to run validation, e.g.\n",
            "          `validation_freq=[1, 2, 10]` runs validation at the end of the\n",
            "          1st, 2nd, and 10th epochs.\n",
            "        max_queue_size: Integer. Used for generator or\n",
            "          `keras.utils.Sequence` input only. Maximum size for the generator\n",
            "          queue.  If unspecified, `max_queue_size` will default to 10.\n",
            "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            "            only. Maximum number of processes to spin up\n",
            "            when using process-based threading. If unspecified, `workers`\n",
            "            will default to 1.\n",
            "        use_multiprocessing: Boolean. Used for generator or\n",
            "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
            "            threading. If unspecified, `use_multiprocessing` will default to\n",
            "            `False`. Note that because this implementation relies on\n",
            "            multiprocessing, you should not pass non-pickleable arguments to\n",
            "            the generator as they can't be passed easily to children\n",
            "            processes.\n",
            "    \n",
            "    Unpacking behavior for iterator-like inputs:\n",
            "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
            "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
            "      yield not only features (x) but optionally targets (y) and sample\n",
            "      weights.  Keras requires that the output of such iterator-likes be\n",
            "      unambiguous. The iterator should return a tuple of length 1, 2, or 3,\n",
            "      where the optional second and third elements will be used for y and\n",
            "      sample_weight respectively. Any other type provided will be wrapped in\n",
            "      a length one tuple, effectively treating everything as 'x'. When\n",
            "      yielding dicts, they should still adhere to the top-level tuple\n",
            "      structure.\n",
            "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
            "      features, targets, and weights from the keys of a single dict.\n",
            "        A notable unsupported data type is the namedtuple. The reason is\n",
            "      that it behaves like both an ordered datatype (tuple) and a mapping\n",
            "      datatype (dict). So given a namedtuple of the form:\n",
            "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
            "      it is ambiguous whether to reverse the order of the elements when\n",
            "      interpreting the value. Even worse is a tuple of the form:\n",
            "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
            "      where it is unclear if the tuple was intended to be unpacked into x,\n",
            "      y, and sample_weight or passed through as a single element to `x`. As\n",
            "      a result the data processing code will simply raise a ValueError if it\n",
            "      encounters a namedtuple. (Along with instructions to remedy the\n",
            "      issue.)\n",
            "    \n",
            "    Returns:\n",
            "        A `History` object. Its `History.history` attribute is\n",
            "        a record of training loss values and metrics values\n",
            "        at successive epochs, as well as validation loss values\n",
            "        and validation metrics values (if applicable).\n",
            "    \n",
            "    Raises:\n",
            "        RuntimeError: 1. If the model was never compiled or,\n",
            "        2. If `model.fit` is  wrapped in `tf.function`.\n",
            "    \n",
            "        ValueError: In case of mismatch between the provided input data\n",
            "            and what the model expects or when the input data is empty.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(model.compile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERL3A12TDvJM",
        "outputId": "bf92c50b-0f86-4d24-a063-6c2f4b1ea032"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method compile in module keras.src.engine.training:\n",
            "\n",
            "compile(optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None, jit_compile=None, pss_evaluation_shards=0, **kwargs) method of keras.src.engine.sequential.Sequential instance\n",
            "    Configures the model for training.\n",
            "    \n",
            "    Example:\n",
            "    \n",
            "    ```python\n",
            "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
            "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
            "                  metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
            "                           tf.keras.metrics.FalseNegatives()])\n",
            "    ```\n",
            "    \n",
            "    Args:\n",
            "        optimizer: String (name of optimizer) or optimizer instance. See\n",
            "          `tf.keras.optimizers`.\n",
            "        loss: Loss function. May be a string (name of loss function), or\n",
            "          a `tf.keras.losses.Loss` instance. See `tf.keras.losses`. A loss\n",
            "          function is any callable with the signature `loss = fn(y_true,\n",
            "          y_pred)`, where `y_true` are the ground truth values, and\n",
            "          `y_pred` are the model's predictions.\n",
            "          `y_true` should have shape\n",
            "          `(batch_size, d0, .. dN)` (except in the case of\n",
            "          sparse loss functions such as\n",
            "          sparse categorical crossentropy which expects integer arrays of\n",
            "          shape `(batch_size, d0, .. dN-1)`).\n",
            "          `y_pred` should have shape `(batch_size, d0, .. dN)`.\n",
            "          The loss function should return a float tensor.\n",
            "          If a custom `Loss` instance is\n",
            "          used and reduction is set to `None`, return value has shape\n",
            "          `(batch_size, d0, .. dN-1)` i.e. per-sample or per-timestep loss\n",
            "          values; otherwise, it is a scalar. If the model has multiple\n",
            "          outputs, you can use a different loss on each output by passing a\n",
            "          dictionary or a list of losses. The loss value that will be\n",
            "          minimized by the model will then be the sum of all individual\n",
            "          losses, unless `loss_weights` is specified.\n",
            "        metrics: List of metrics to be evaluated by the model during\n",
            "          training and testing. Each of this can be a string (name of a\n",
            "          built-in function), function or a `tf.keras.metrics.Metric`\n",
            "          instance. See `tf.keras.metrics`. Typically you will use\n",
            "          `metrics=['accuracy']`.\n",
            "          A function is any callable with the signature `result = fn(y_true,\n",
            "          y_pred)`. To specify different metrics for different outputs of a\n",
            "          multi-output model, you could also pass a dictionary, such as\n",
            "          `metrics={'output_a':'accuracy', 'output_b':['accuracy', 'mse']}`.\n",
            "          You can also pass a list to specify a metric or a list of metrics\n",
            "          for each output, such as\n",
            "          `metrics=[['accuracy'], ['accuracy', 'mse']]`\n",
            "          or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
            "          strings 'accuracy' or 'acc', we convert this to one of\n",
            "          `tf.keras.metrics.BinaryAccuracy`,\n",
            "          `tf.keras.metrics.CategoricalAccuracy`,\n",
            "          `tf.keras.metrics.SparseCategoricalAccuracy` based on the shapes\n",
            "          of the targets and of the model output. We do a similar\n",
            "          conversion for the strings 'crossentropy' and 'ce' as well.\n",
            "          The metrics passed here are evaluated without sample weighting; if\n",
            "          you would like sample weighting to apply, you can specify your\n",
            "          metrics via the `weighted_metrics` argument instead.\n",
            "        loss_weights: Optional list or dictionary specifying scalar\n",
            "          coefficients (Python floats) to weight the loss contributions of\n",
            "          different model outputs. The loss value that will be minimized by\n",
            "          the model will then be the *weighted sum* of all individual\n",
            "          losses, weighted by the `loss_weights` coefficients.  If a list,\n",
            "          it is expected to have a 1:1 mapping to the model's outputs. If a\n",
            "          dict, it is expected to map output names (strings) to scalar\n",
            "          coefficients.\n",
            "        weighted_metrics: List of metrics to be evaluated and weighted by\n",
            "          `sample_weight` or `class_weight` during training and testing.\n",
            "        run_eagerly: Bool. If `True`, this `Model`'s logic will not be\n",
            "          wrapped in a `tf.function`. Recommended to leave this as `None`\n",
            "          unless your `Model` cannot be run inside a `tf.function`.\n",
            "          `run_eagerly=True` is not supported when using\n",
            "          `tf.distribute.experimental.ParameterServerStrategy`. Defaults to\n",
            "           `False`.\n",
            "        steps_per_execution: Int or `'auto'`. The number of batches to\n",
            "          run during each `tf.function` call. If set to \"auto\", keras will\n",
            "          automatically tune `steps_per_execution` during runtime. Running\n",
            "          multiple batches inside a single `tf.function` call can greatly\n",
            "          improve performance on TPUs, when used with distributed strategies\n",
            "          such as `ParameterServerStrategy`, or with small models with a\n",
            "          large Python overhead. At most, one full epoch will be run each\n",
            "          execution. If a number larger than the size of the epoch is\n",
            "          passed, the execution will be truncated to the size of the epoch.\n",
            "          Note that if `steps_per_execution` is set to `N`,\n",
            "          `Callback.on_batch_begin` and `Callback.on_batch_end` methods will\n",
            "          only be called every `N` batches (i.e. before/after each\n",
            "          `tf.function` execution). Defaults to `1`.\n",
            "        jit_compile: If `True`, compile the model training step with XLA.\n",
            "          [XLA](https://www.tensorflow.org/xla) is an optimizing compiler\n",
            "          for machine learning.\n",
            "          `jit_compile` is not enabled for by default.\n",
            "          Note that `jit_compile=True`\n",
            "          may not necessarily work for all models.\n",
            "          For more information on supported operations please refer to the\n",
            "          [XLA documentation](https://www.tensorflow.org/xla).\n",
            "          Also refer to\n",
            "          [known XLA issues](https://www.tensorflow.org/xla/known_issues)\n",
            "          for more details.\n",
            "        pss_evaluation_shards: Integer or 'auto'. Used for\n",
            "          `tf.distribute.ParameterServerStrategy` training only. This arg\n",
            "          sets the number of shards to split the dataset into, to enable an\n",
            "          exact visitation guarantee for evaluation, meaning the model will\n",
            "          be applied to each dataset element exactly once, even if workers\n",
            "          fail. The dataset must be sharded to ensure separate workers do\n",
            "          not process the same data. The number of shards should be at least\n",
            "          the number of workers for good performance. A value of 'auto'\n",
            "          turns on exact evaluation and uses a heuristic for the number of\n",
            "          shards based on the number of workers. 0, meaning no\n",
            "          visitation guarantee is provided. NOTE: Custom implementations of\n",
            "          `Model.test_step` will be ignored when doing exact evaluation.\n",
            "          Defaults to `0`.\n",
            "        **kwargs: Arguments supported for backwards compatibility only.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(SGD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFzmTRxWD_CJ",
        "outputId": "d325c191-fdde-4c62-8e09-4c72b7c4e9cf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class SGD in module keras.src.optimizers.sgd:\n",
            "\n",
            "class SGD(keras.src.optimizers.optimizer.Optimizer)\n",
            " |  SGD(learning_rate=0.01, momentum=0.0, nesterov=False, weight_decay=None, clipnorm=None, clipvalue=None, global_clipnorm=None, use_ema=False, ema_momentum=0.99, ema_overwrite_frequency=None, jit_compile=True, name='SGD', **kwargs)\n",
            " |  \n",
            " |  Gradient descent (with momentum) optimizer.\n",
            " |  \n",
            " |  Update rule for parameter `w` with gradient `g` when `momentum` is 0:\n",
            " |  \n",
            " |  ```python\n",
            " |  w = w - learning_rate * g\n",
            " |  ```\n",
            " |  \n",
            " |  Update rule when `momentum` is larger than 0:\n",
            " |  \n",
            " |  ```python\n",
            " |  velocity = momentum * velocity - learning_rate * g\n",
            " |  w = w + velocity\n",
            " |  ```\n",
            " |  \n",
            " |  When `nesterov=True`, this rule becomes:\n",
            " |  \n",
            " |  ```python\n",
            " |  velocity = momentum * velocity - learning_rate * g\n",
            " |  w = w + momentum * velocity - learning_rate * g\n",
            " |  ```\n",
            " |  \n",
            " |  Args:\n",
            " |      learning_rate: A `Tensor`, floating point value, or a schedule that is a\n",
            " |          `keras.optimizers.schedules.LearningRateSchedule`, or a callable\n",
            " |          that takes no arguments and returns the actual value to use. The\n",
            " |          learning rate. Defaults to 0.001.\n",
            " |      momentum: float hyperparameter >= 0 that accelerates gradient descent in\n",
            " |          the relevant direction and dampens oscillations.\n",
            " |          Defaults to 0, i.e., vanilla gradient descent.\n",
            " |      nesterov: boolean. Whether to apply Nesterov momentum.\n",
            " |          Defaults to `False`.\n",
            " |    name: String. The name to use\n",
            " |        for momentum accumulator weights created by\n",
            " |        the optimizer.\n",
            " |    weight_decay: Float, defaults to None. If set, weight decay is applied.\n",
            " |    clipnorm: Float. If set, the gradient of each weight is individually\n",
            " |        clipped so that its norm is no higher than this value.\n",
            " |    clipvalue: Float. If set, the gradient of each weight is clipped to be no\n",
            " |        higher than this value.\n",
            " |    global_clipnorm: Float. If set, the gradient of all weights is clipped so\n",
            " |        that their global norm is no higher than this value.\n",
            " |    use_ema: Boolean, defaults to False. If True, exponential moving average\n",
            " |        (EMA) is applied. EMA consists of computing an exponential moving\n",
            " |        average of the weights of the model (as the weight values change after\n",
            " |        each training batch), and periodically overwriting the weights with\n",
            " |        their moving average.\n",
            " |    ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.\n",
            " |        This is the momentum to use when computing\n",
            " |        the EMA of the model's weights:\n",
            " |        `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n",
            " |        current_variable_value`.\n",
            " |    ema_overwrite_frequency: Int or None, defaults to None. Only used if\n",
            " |        `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,\n",
            " |        we overwrite the model variable by its moving average.\n",
            " |        If None, the optimizer\n",
            " |        does not overwrite model variables in the middle of training, and you\n",
            " |        need to explicitly overwrite the variables at the end of training\n",
            " |        by calling `optimizer.finalize_variable_values()`\n",
            " |        (which updates the model\n",
            " |        variables in-place). When using the built-in `fit()` training loop,\n",
            " |        this happens automatically after the last epoch,\n",
            " |        and you don't need to do anything.\n",
            " |    jit_compile: Boolean, defaults to True.\n",
            " |        If True, the optimizer will use XLA\n",
            " |        compilation. If no GPU device is found, this flag will be ignored.\n",
            " |    mesh: optional `tf.experimental.dtensor.Mesh` instance. When provided,\n",
            " |        the optimizer will be run in DTensor mode, e.g. state\n",
            " |        tracking variable will be a DVariable, and aggregation/reduction will\n",
            " |        happen in the global DTensor context.\n",
            " |    **kwargs: keyword arguments only used for backward compatibility.\n",
            " |  \n",
            " |  Usage:\n",
            " |  \n",
            " |  >>> opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
            " |  >>> var = tf.Variable(1.0)\n",
            " |  >>> loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1\n",
            " |  >>> opt.minimize(loss, [var])\n",
            " |  >>> # Step is `- learning_rate * grad`\n",
            " |  >>> var.numpy()\n",
            " |  0.9\n",
            " |  \n",
            " |  >>> opt = tf.keras.optimizers.SGD(0.1, momentum=0.9)\n",
            " |  >>> var = tf.Variable(1.0)\n",
            " |  >>> val0 = var.value()\n",
            " |  >>> loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1\n",
            " |  >>> # First step is `- learning_rate * grad`\n",
            " |  >>> opt.minimize(loss, [var])\n",
            " |  >>> val1 = var.value()\n",
            " |  >>> (val0 - val1).numpy()\n",
            " |  0.1\n",
            " |  >>> # On later steps, step-size increases because of momentum\n",
            " |  >>> opt.minimize(loss, [var])\n",
            " |  >>> val2 = var.value()\n",
            " |  >>> (val1 - val2).numpy()\n",
            " |  0.18\n",
            " |  \n",
            " |  Reference:\n",
            " |      - For `nesterov=True`, See [Sutskever et al., 2013](\n",
            " |        http://proceedings.mlr.press/v28/sutskever13.pdf).\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      SGD\n",
            " |      keras.src.optimizers.optimizer.Optimizer\n",
            " |      keras.src.optimizers.optimizer._BaseOptimizer\n",
            " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
            " |      tensorflow.python.trackable.base.Trackable\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, learning_rate=0.01, momentum=0.0, nesterov=False, weight_decay=None, clipnorm=None, clipvalue=None, global_clipnorm=None, use_ema=False, ema_momentum=0.99, ema_overwrite_frequency=None, jit_compile=True, name='SGD', **kwargs)\n",
            " |      Create a new Optimizer.\n",
            " |  \n",
            " |  build(self, var_list)\n",
            " |      Initialize optimizer variables.\n",
            " |      \n",
            " |      SGD optimizer has one variable `momentums`, only set if `self.momentum`\n",
            " |      is not 0.\n",
            " |      \n",
            " |      Args:\n",
            " |        var_list: list of model variables to build SGD variables on.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the optimizer.\n",
            " |      \n",
            " |      An optimizer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of an optimizer.\n",
            " |      The same optimizer can be reinstantiated later\n",
            " |      (without any saved state) from this configuration.\n",
            " |      \n",
            " |      Subclass optimizer should override this method to include other\n",
            " |      hyperparameters.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  update_step(self, gradient, variable)\n",
            " |      Update step given gradient and the associated model variable.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.src.optimizers.optimizer.Optimizer:\n",
            " |  \n",
            " |  add_variable_from_reference(self, model_variable, variable_name, shape=None, initial_value=None)\n",
            " |      Create an optimizer variable from model variable.\n",
            " |      \n",
            " |      Create an optimizer variable based on the information of model variable.\n",
            " |      For example, in SGD optimizer momemtum, for each model variable, a\n",
            " |      corresponding momemtum variable is created of the same shape and dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        model_variable: tf.Variable. The corresponding model variable to the\n",
            " |          optimizer variable to be created.\n",
            " |        variable_name: String. The name prefix of the optimizer variable to be\n",
            " |          created. The create variables name will follow the pattern\n",
            " |          `{variable_name}/{model_variable.name}`, e.g., `momemtum/dense_1`.\n",
            " |        shape: List or Tuple, defaults to None. The shape of the optimizer\n",
            " |          variable to be created. If None, the created variable will have the\n",
            " |          same shape as `model_variable`.\n",
            " |        initial_value: A Tensor, or Python object convertible to a Tensor,\n",
            " |          defaults to None. The initial value of the optimizer variable, if\n",
            " |          None, the initial value will be default to 0.\n",
            " |      \n",
            " |      Returns:\n",
            " |        An optimizer variable.\n",
            " |  \n",
            " |  aggregate_gradients(self, grads_and_vars)\n",
            " |      Aggregate gradients on all devices.\n",
            " |      \n",
            " |      By default, we will perform reduce_sum of gradients across devices.\n",
            " |      Users can implement their own aggregation logic by overriding this\n",
            " |      method.\n",
            " |      \n",
            " |      Args:\n",
            " |        grads_and_vars: List of (gradient, variable) pairs.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of (gradient, variable) pairs.\n",
            " |  \n",
            " |  apply_gradients(self, grads_and_vars, name=None, skip_gradients_aggregation=False, **kwargs)\n",
            " |      Apply gradients to variables.\n",
            " |      \n",
            " |      Args:\n",
            " |        grads_and_vars: List of `(gradient, variable)` pairs.\n",
            " |        name: string, defaults to None. The name of the namescope to\n",
            " |          use when creating variables. If None, `self.name` will be used.\n",
            " |        skip_gradients_aggregation: If true, gradients aggregation will not be\n",
            " |          performed inside optimizer. Usually this arg is set to True when you\n",
            " |          write custom code aggregating gradients outside the optimizer.\n",
            " |        **kwargs: keyword arguments only used for backward compatibility.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `tf.Variable`, representing the current iteration.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If `grads_and_vars` is malformed.\n",
            " |        RuntimeError: If called in a cross-replica context.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.src.optimizers.optimizer._BaseOptimizer:\n",
            " |  \n",
            " |  add_variable(self, shape, dtype=None, initializer='zeros', name=None)\n",
            " |      Create an optimizer variable.\n",
            " |      \n",
            " |      Args:\n",
            " |        shape: A list of integers, a tuple of integers, or a 1-D Tensor of\n",
            " |          type int32. Defaults to scalar if unspecified.\n",
            " |        dtype: The DType of the optimizer variable to be created. Defaults to\n",
            " |          `tf.keras.backend.floatx` if unspecified.\n",
            " |        initializer: string or callable. Initializer instance.\n",
            " |        name: The name of the optimizer variable to be created.\n",
            " |      \n",
            " |      Returns:\n",
            " |        An optimizer variable, in the format of tf.Variable.\n",
            " |  \n",
            " |  compute_gradients(self, loss, var_list, tape=None)\n",
            " |      Compute gradients of loss on trainable variables.\n",
            " |      \n",
            " |      Args:\n",
            " |        loss: `Tensor` or callable. If a callable, `loss` should take no\n",
            " |          arguments and return the value to minimize.\n",
            " |        var_list: list or tuple of `Variable` objects to update to minimize\n",
            " |          `loss`, or a callable returning the list or tuple of `Variable`\n",
            " |          objects. Use callable when the variable list would otherwise be\n",
            " |          incomplete before `minimize` since the variables are created at the\n",
            " |          first time `loss` is called.\n",
            " |        tape: (Optional) `tf.GradientTape`. If `loss` is provided as a\n",
            " |          `Tensor`, the tape that computed the `loss` must be provided.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of (gradient, variable) pairs. Variable is always present, but\n",
            " |        gradient can be `None`.\n",
            " |  \n",
            " |  exclude_from_weight_decay(self, var_list=None, var_names=None)\n",
            " |      Exclude variables from weight decay.\n",
            " |      \n",
            " |      This method must be called before the optimizer's `build` method is\n",
            " |      called. You can set specific variables to exclude out, or set a list of\n",
            " |      strings as the anchor words, if any of which appear in a variable's\n",
            " |      name, then the variable is excluded.\n",
            " |      \n",
            " |      Args:\n",
            " |          var_list: A list of `tf.Variable`s to exclude from weight decay.\n",
            " |          var_names: A list of strings. If any string in `var_names` appear\n",
            " |              in the model variable's name, then this model variable is\n",
            " |              excluded from weight decay. For example, `var_names=['bias']`\n",
            " |              excludes all bias variables from weight decay.\n",
            " |  \n",
            " |  finalize_variable_values(self, var_list)\n",
            " |      Set the final value of model's trainable variables.\n",
            " |      \n",
            " |      Sometimes there are some extra steps before ending the variable updates,\n",
            " |      such as overriding the model variables with its average value.\n",
            " |      \n",
            " |      Args:\n",
            " |        var_list: list of model variables.\n",
            " |  \n",
            " |  load_own_variables(self, store)\n",
            " |      Set the state of this optimizer object.\n",
            " |  \n",
            " |  minimize(self, loss, var_list, tape=None)\n",
            " |      Minimize `loss` by updating `var_list`.\n",
            " |      \n",
            " |      This method simply computes gradient using `tf.GradientTape` and calls\n",
            " |      `apply_gradients()`. If you want to process the gradient before applying\n",
            " |      then call `tf.GradientTape` and `apply_gradients()` explicitly instead\n",
            " |      of using this function.\n",
            " |      \n",
            " |      Args:\n",
            " |        loss: `Tensor` or callable. If a callable, `loss` should take no\n",
            " |          arguments and return the value to minimize.\n",
            " |        var_list: list or tuple of `Variable` objects to update to minimize\n",
            " |          `loss`, or a callable returning the list or tuple of `Variable`\n",
            " |          objects.  Use callable when the variable list would otherwise be\n",
            " |          incomplete before `minimize` since the variables are created at the\n",
            " |          first time `loss` is called.\n",
            " |        tape: (Optional) `tf.GradientTape`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  save_own_variables(self, store)\n",
            " |      Get the state of this optimizer object.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Set the weights of the optimizer.\n",
            " |      \n",
            " |      Args:\n",
            " |          weights: a list of `tf.Variable`s or numpy arrays, the target values\n",
            " |              of optimizer variables. It should have the same order as\n",
            " |              `self._variables`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from keras.src.optimizers.optimizer._BaseOptimizer:\n",
            " |  \n",
            " |  from_config(config, custom_objects=None) from builtins.type\n",
            " |      Creates an optimizer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`, capable of instantiating the\n",
            " |      same optimizer from the config dictionary.\n",
            " |      \n",
            " |      Args:\n",
            " |          config: A Python dictionary, typically the output of get_config.\n",
            " |          custom_objects: A Python dictionary mapping names to additional\n",
            " |            user-defined Python objects needed to recreate this optimizer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An optimizer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from keras.src.optimizers.optimizer._BaseOptimizer:\n",
            " |  \n",
            " |  variables\n",
            " |      Returns variables of this optimizer.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.src.optimizers.optimizer._BaseOptimizer:\n",
            " |  \n",
            " |  iterations\n",
            " |      The number of training steps this `optimizer` has run.\n",
            " |      \n",
            " |      By default, iterations would be incremented by one every time\n",
            " |      `apply_gradients()` is called.\n",
            " |  \n",
            " |  learning_rate\n",
            " |  \n",
            " |  lr\n",
            " |      Alias of `learning_rate()`.\n",
            " |      \n",
            " |      `lr()` is heavily called in workflows using `optimizer_v2.OptimizerV2`,\n",
            " |      so we keep it for backward compabitliy.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow.python.trackable.autotrackable.AutoTrackable:\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAzizJVJHew1",
        "outputId": "85244a3d-bff1-4acf-a5c2-8933ec6d2c9d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               200960    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 235146 (918.54 KB)\n",
            "Trainable params: 235146 (918.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "60000 * 0.9 / 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1YUu330IQHB",
        "outputId": "bd081c7b-b588-46bd-c000-5dc9e15e9df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "421.875"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sgd = SGD(0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
        "H = model.fit(trainX, trainY, validation_split=0.1,epochs=100, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_hun8OZ3Uru",
        "outputId": "f92e4544-b3af-47af-fd02-9dc0969f9b0c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "422/422 [==============================] - 4s 4ms/step - loss: 2.2804 - accuracy: 0.1872 - val_loss: 2.2496 - val_accuracy: 0.2493\n",
            "Epoch 2/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 2.2221 - accuracy: 0.3564 - val_loss: 2.1849 - val_accuracy: 0.4323\n",
            "Epoch 3/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 2.1514 - accuracy: 0.4666 - val_loss: 2.1010 - val_accuracy: 0.5622\n",
            "Epoch 4/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 2.0547 - accuracy: 0.5362 - val_loss: 1.9846 - val_accuracy: 0.5610\n",
            "Epoch 5/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 1.9203 - accuracy: 0.5825 - val_loss: 1.8215 - val_accuracy: 0.6252\n",
            "Epoch 6/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 1.7467 - accuracy: 0.6267 - val_loss: 1.6255 - val_accuracy: 0.6932\n",
            "Epoch 7/100\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 1.5508 - accuracy: 0.6729 - val_loss: 1.4182 - val_accuracy: 0.7317\n",
            "Epoch 8/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 1.3593 - accuracy: 0.7109 - val_loss: 1.2292 - val_accuracy: 0.7690\n",
            "Epoch 9/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 1.1934 - accuracy: 0.7409 - val_loss: 1.0719 - val_accuracy: 0.7918\n",
            "Epoch 10/100\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 1.0599 - accuracy: 0.7609 - val_loss: 0.9473 - val_accuracy: 0.8103\n",
            "Epoch 11/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.9560 - accuracy: 0.7795 - val_loss: 0.8511 - val_accuracy: 0.8260\n",
            "Epoch 12/100\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.8743 - accuracy: 0.7913 - val_loss: 0.7745 - val_accuracy: 0.8390\n",
            "Epoch 13/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.8092 - accuracy: 0.8033 - val_loss: 0.7131 - val_accuracy: 0.8492\n",
            "Epoch 14/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.7559 - accuracy: 0.8125 - val_loss: 0.6626 - val_accuracy: 0.8530\n",
            "Epoch 15/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.7115 - accuracy: 0.8202 - val_loss: 0.6198 - val_accuracy: 0.8588\n",
            "Epoch 16/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.6736 - accuracy: 0.8280 - val_loss: 0.5838 - val_accuracy: 0.8635\n",
            "Epoch 17/100\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.6410 - accuracy: 0.8350 - val_loss: 0.5527 - val_accuracy: 0.8687\n",
            "Epoch 18/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.6129 - accuracy: 0.8406 - val_loss: 0.5263 - val_accuracy: 0.8738\n",
            "Epoch 19/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.5880 - accuracy: 0.8456 - val_loss: 0.5024 - val_accuracy: 0.8795\n",
            "Epoch 20/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.5660 - accuracy: 0.8510 - val_loss: 0.4821 - val_accuracy: 0.8828\n",
            "Epoch 21/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.5465 - accuracy: 0.8548 - val_loss: 0.4634 - val_accuracy: 0.8868\n",
            "Epoch 22/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.5289 - accuracy: 0.8590 - val_loss: 0.4470 - val_accuracy: 0.8917\n",
            "Epoch 23/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.5132 - accuracy: 0.8627 - val_loss: 0.4326 - val_accuracy: 0.8930\n",
            "Epoch 24/100\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.4989 - accuracy: 0.8660 - val_loss: 0.4197 - val_accuracy: 0.8963\n",
            "Epoch 25/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.4861 - accuracy: 0.8692 - val_loss: 0.4077 - val_accuracy: 0.8980\n",
            "Epoch 26/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.4744 - accuracy: 0.8715 - val_loss: 0.3969 - val_accuracy: 0.9015\n",
            "Epoch 27/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.4637 - accuracy: 0.8744 - val_loss: 0.3880 - val_accuracy: 0.9037\n",
            "Epoch 28/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.4539 - accuracy: 0.8768 - val_loss: 0.3787 - val_accuracy: 0.9047\n",
            "Epoch 29/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.4450 - accuracy: 0.8791 - val_loss: 0.3705 - val_accuracy: 0.9052\n",
            "Epoch 30/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.4369 - accuracy: 0.8810 - val_loss: 0.3631 - val_accuracy: 0.9067\n",
            "Epoch 31/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.4293 - accuracy: 0.8826 - val_loss: 0.3565 - val_accuracy: 0.9080\n",
            "Epoch 32/100\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.4224 - accuracy: 0.8848 - val_loss: 0.3509 - val_accuracy: 0.9070\n",
            "Epoch 33/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.4160 - accuracy: 0.8863 - val_loss: 0.3449 - val_accuracy: 0.9087\n",
            "Epoch 34/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.4099 - accuracy: 0.8875 - val_loss: 0.3395 - val_accuracy: 0.9102\n",
            "Epoch 35/100\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 0.4045 - accuracy: 0.8886 - val_loss: 0.3353 - val_accuracy: 0.9097\n",
            "Epoch 36/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3994 - accuracy: 0.8897 - val_loss: 0.3303 - val_accuracy: 0.9112\n",
            "Epoch 37/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3945 - accuracy: 0.8910 - val_loss: 0.3258 - val_accuracy: 0.9115\n",
            "Epoch 38/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3901 - accuracy: 0.8916 - val_loss: 0.3221 - val_accuracy: 0.9125\n",
            "Epoch 39/100\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.3859 - accuracy: 0.8924 - val_loss: 0.3184 - val_accuracy: 0.9135\n",
            "Epoch 40/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3819 - accuracy: 0.8931 - val_loss: 0.3149 - val_accuracy: 0.9150\n",
            "Epoch 41/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3782 - accuracy: 0.8941 - val_loss: 0.3118 - val_accuracy: 0.9147\n",
            "Epoch 42/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3746 - accuracy: 0.8950 - val_loss: 0.3090 - val_accuracy: 0.9160\n",
            "Epoch 43/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3713 - accuracy: 0.8952 - val_loss: 0.3056 - val_accuracy: 0.9152\n",
            "Epoch 44/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3681 - accuracy: 0.8963 - val_loss: 0.3029 - val_accuracy: 0.9157\n",
            "Epoch 45/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3650 - accuracy: 0.8970 - val_loss: 0.3003 - val_accuracy: 0.9162\n",
            "Epoch 46/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3621 - accuracy: 0.8976 - val_loss: 0.2984 - val_accuracy: 0.9170\n",
            "Epoch 47/100\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.3594 - accuracy: 0.8982 - val_loss: 0.2954 - val_accuracy: 0.9168\n",
            "Epoch 48/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3567 - accuracy: 0.8985 - val_loss: 0.2929 - val_accuracy: 0.9185\n",
            "Epoch 49/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3542 - accuracy: 0.8994 - val_loss: 0.2912 - val_accuracy: 0.9188\n",
            "Epoch 50/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3518 - accuracy: 0.8995 - val_loss: 0.2889 - val_accuracy: 0.9190\n",
            "Epoch 51/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3494 - accuracy: 0.8999 - val_loss: 0.2868 - val_accuracy: 0.9195\n",
            "Epoch 52/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3471 - accuracy: 0.9011 - val_loss: 0.2847 - val_accuracy: 0.9198\n",
            "Epoch 53/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3449 - accuracy: 0.9014 - val_loss: 0.2833 - val_accuracy: 0.9200\n",
            "Epoch 54/100\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.3428 - accuracy: 0.9024 - val_loss: 0.2811 - val_accuracy: 0.9210\n",
            "Epoch 55/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3408 - accuracy: 0.9024 - val_loss: 0.2795 - val_accuracy: 0.9202\n",
            "Epoch 56/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3388 - accuracy: 0.9031 - val_loss: 0.2780 - val_accuracy: 0.9210\n",
            "Epoch 57/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3369 - accuracy: 0.9031 - val_loss: 0.2760 - val_accuracy: 0.9208\n",
            "Epoch 58/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3349 - accuracy: 0.9040 - val_loss: 0.2747 - val_accuracy: 0.9207\n",
            "Epoch 59/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3331 - accuracy: 0.9047 - val_loss: 0.2733 - val_accuracy: 0.9210\n",
            "Epoch 60/100\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 0.3313 - accuracy: 0.9051 - val_loss: 0.2716 - val_accuracy: 0.9223\n",
            "Epoch 61/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3296 - accuracy: 0.9057 - val_loss: 0.2700 - val_accuracy: 0.9223\n",
            "Epoch 62/100\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.3279 - accuracy: 0.9060 - val_loss: 0.2685 - val_accuracy: 0.9212\n",
            "Epoch 63/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3263 - accuracy: 0.9064 - val_loss: 0.2672 - val_accuracy: 0.9220\n",
            "Epoch 64/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3246 - accuracy: 0.9069 - val_loss: 0.2659 - val_accuracy: 0.9227\n",
            "Epoch 65/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3231 - accuracy: 0.9074 - val_loss: 0.2645 - val_accuracy: 0.9242\n",
            "Epoch 66/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3216 - accuracy: 0.9078 - val_loss: 0.2633 - val_accuracy: 0.9247\n",
            "Epoch 67/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3201 - accuracy: 0.9080 - val_loss: 0.2620 - val_accuracy: 0.9247\n",
            "Epoch 68/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3186 - accuracy: 0.9086 - val_loss: 0.2606 - val_accuracy: 0.9250\n",
            "Epoch 69/100\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.3171 - accuracy: 0.9084 - val_loss: 0.2601 - val_accuracy: 0.9255\n",
            "Epoch 70/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3158 - accuracy: 0.9091 - val_loss: 0.2584 - val_accuracy: 0.9257\n",
            "Epoch 71/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3144 - accuracy: 0.9096 - val_loss: 0.2568 - val_accuracy: 0.9265\n",
            "Epoch 72/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3130 - accuracy: 0.9101 - val_loss: 0.2567 - val_accuracy: 0.9267\n",
            "Epoch 73/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3117 - accuracy: 0.9105 - val_loss: 0.2549 - val_accuracy: 0.9270\n",
            "Epoch 74/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3103 - accuracy: 0.9106 - val_loss: 0.2536 - val_accuracy: 0.9278\n",
            "Epoch 75/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3091 - accuracy: 0.9112 - val_loss: 0.2531 - val_accuracy: 0.9282\n",
            "Epoch 76/100\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.3078 - accuracy: 0.9113 - val_loss: 0.2517 - val_accuracy: 0.9283\n",
            "Epoch 77/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3065 - accuracy: 0.9118 - val_loss: 0.2510 - val_accuracy: 0.9290\n",
            "Epoch 78/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3053 - accuracy: 0.9121 - val_loss: 0.2500 - val_accuracy: 0.9270\n",
            "Epoch 79/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3041 - accuracy: 0.9124 - val_loss: 0.2486 - val_accuracy: 0.9275\n",
            "Epoch 80/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3029 - accuracy: 0.9125 - val_loss: 0.2480 - val_accuracy: 0.9285\n",
            "Epoch 81/100\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 0.3018 - accuracy: 0.9133 - val_loss: 0.2466 - val_accuracy: 0.9285\n",
            "Epoch 82/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3005 - accuracy: 0.9136 - val_loss: 0.2456 - val_accuracy: 0.9285\n",
            "Epoch 83/100\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.2994 - accuracy: 0.9138 - val_loss: 0.2449 - val_accuracy: 0.9280\n",
            "Epoch 84/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2984 - accuracy: 0.9143 - val_loss: 0.2438 - val_accuracy: 0.9297\n",
            "Epoch 85/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2972 - accuracy: 0.9146 - val_loss: 0.2432 - val_accuracy: 0.9297\n",
            "Epoch 86/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2962 - accuracy: 0.9143 - val_loss: 0.2422 - val_accuracy: 0.9295\n",
            "Epoch 87/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2951 - accuracy: 0.9149 - val_loss: 0.2411 - val_accuracy: 0.9293\n",
            "Epoch 88/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2941 - accuracy: 0.9149 - val_loss: 0.2404 - val_accuracy: 0.9307\n",
            "Epoch 89/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2930 - accuracy: 0.9158 - val_loss: 0.2397 - val_accuracy: 0.9292\n",
            "Epoch 90/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2919 - accuracy: 0.9155 - val_loss: 0.2388 - val_accuracy: 0.9302\n",
            "Epoch 91/100\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.2909 - accuracy: 0.9162 - val_loss: 0.2381 - val_accuracy: 0.9308\n",
            "Epoch 92/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2899 - accuracy: 0.9164 - val_loss: 0.2381 - val_accuracy: 0.9305\n",
            "Epoch 93/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2890 - accuracy: 0.9165 - val_loss: 0.2362 - val_accuracy: 0.9308\n",
            "Epoch 94/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2880 - accuracy: 0.9171 - val_loss: 0.2354 - val_accuracy: 0.9307\n",
            "Epoch 95/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2870 - accuracy: 0.9169 - val_loss: 0.2348 - val_accuracy: 0.9310\n",
            "Epoch 96/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2859 - accuracy: 0.9175 - val_loss: 0.2341 - val_accuracy: 0.9315\n",
            "Epoch 97/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2850 - accuracy: 0.9177 - val_loss: 0.2331 - val_accuracy: 0.9317\n",
            "Epoch 98/100\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.2841 - accuracy: 0.9184 - val_loss: 0.2323 - val_accuracy: 0.9320\n",
            "Epoch 99/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2831 - accuracy: 0.9183 - val_loss: 0.2320 - val_accuracy: 0.9323\n",
            "Epoch 100/100\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2822 - accuracy: 0.9187 - val_loss: 0.2309 - val_accuracy: 0.9322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(testX, batch_size=128)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "    predictions.argmax(axis=1),\n",
        "    target_names=[str(x) for x in lb.classes_]))"
      ],
      "metadata": {
        "id": "aKDns3tM3WgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9621949c-678f-4595-94aa-43f1f21c3735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96       980\n",
            "           1       0.97      0.97      0.97      1135\n",
            "           2       0.93      0.90      0.91      1032\n",
            "           3       0.90      0.91      0.91      1010\n",
            "           4       0.92      0.94      0.93       982\n",
            "           5       0.90      0.86      0.88       892\n",
            "           6       0.93      0.94      0.93       958\n",
            "           7       0.93      0.92      0.93      1028\n",
            "           8       0.89      0.89      0.89       974\n",
            "           9       0.91      0.90      0.90      1009\n",
            "\n",
            "    accuracy                           0.92     10000\n",
            "   macro avg       0.92      0.92      0.92     10000\n",
            "weighted avg       0.92      0.92      0.92     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('ggplot')\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, 100), H.history['loss'], label='train_loss')\n",
        "plt.plot(np.arange(0, 100), H.history['val_loss'], label='val_loss')\n",
        "plt.plot(np.arange(0, 100), H.history['accuracy'], label='train_acc')\n",
        "plt.plot(np.arange(0, 100), H.history['val_accuracy'], label='val_acc')\n",
        "plt.title('Training Loss and Accuracy')\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Loss/Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "0_Yd9Iob3aHD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "ed796d51-da65-45d3-e212-e8907527807d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x798232ea0f10>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}